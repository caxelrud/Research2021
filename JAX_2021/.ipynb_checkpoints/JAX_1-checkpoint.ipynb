{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google JAX on Windows\n",
    "#### Celso Axelrud\n",
    "#### Revision 1.0 - 4/24/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document describes the efforts to execute Google JAX (https://github.com/google/jax) on Windows OS.\n",
    "\n",
    "JAX is available for Linux including on Google Collab environment.\n",
    "But most of the client's solutions are implemented on Windows OS.\n",
    "I have been collaborating by trying to compile and test JAX for Windows.\n",
    "\n",
    "Finally, I got it running correctly for the CPU version.\n",
    "I am still working on tests for GPU version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX is much more than just a GPU-backed NumPy.\n",
    "JAX is Autograd and XLA, brought together for high-performance machine learning research.\n",
    "JAX can automatically differentiate native Python and NumPy functions. \n",
    "It can differentiate through loops, branches, recursion, and closures, and it can take \n",
    "derivatives of derivatives of derivatives. \n",
    "It supports reverse-mode differentiation (a.k.a. backpropagation) via grad as well as\n",
    "forward-mode differentiation, and the two can be composed arbitrarily to any order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries on top of JAX\n",
    "My personal interest is on NumPyro (UBER).\n",
    "\n",
    "Other libraries are Flax (Google Brain), Haiku (Deepmind), Trax (Google Brain), Objax (Google), Stax (Google), Elegy (PoetsAI), RLax (Deepmind), Optax (Deepmind), Jraph (Deepmind), JAX-M.D. (Google), Oryx (Google)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4199743\n",
      "0.6216266\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "def predict(params, inputs):\n",
    "  for W, b in params:\n",
    "    outputs = jnp.dot(inputs, W) + b\n",
    "    inputs = jnp.tanh(outputs)\n",
    "  return outputs\n",
    "\n",
    "def logprob_fun(params, inputs, targets):\n",
    "  preds = predict(params, inputs)\n",
    "  return jnp.sum((preds - targets)**2)\n",
    "\n",
    "grad_fun = jit(grad(logprob_fun))  # compiled gradient evaluation function\n",
    "perex_grads = jit(vmap(grad_fun, in_axes=(None, 0, 0)))  # fast per-example grads\n",
    "\n",
    "\n",
    "#from jax import grad\n",
    "\n",
    "def tanh(x):  # Define a function\n",
    "  y = jnp.exp(-2.0 * x)\n",
    "  return (1.0 - y) / (1.0 + y)\n",
    "\n",
    "grad_tanh = grad(tanh)  # Obtain its gradient function\n",
    "print(grad_tanh(1.0))   # Evaluate it at x = 1.0\n",
    "# prints 0.4199743\n",
    "\n",
    "print(grad(grad(grad(tanh)))(1.0))\n",
    "# prints 0.62162673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-1.0\n",
      "The slowest run took 5.49 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "27.8 ms ± 18.4 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "180 ms ± 12.6 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "def hessian(fun):\n",
    "  return jit(jacfwd(jacrev(fun)))\n",
    "\n",
    "def abs_val(x):\n",
    "  if x > 0:\n",
    "    return x\n",
    "  else:\n",
    "    return -x\n",
    "\n",
    "abs_val_grad = grad(abs_val)\n",
    "print(abs_val_grad(1.0))   # prints 1.0\n",
    "print(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "def slow_f(x):\n",
    "  # Element-wise ops see a large benefit from fusion\n",
    "  return x * x + x * 2.0\n",
    "\n",
    "x = jnp.ones((5000, 5000))\n",
    "fast_f = jit(slow_f)\n",
    "%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X\n",
    "%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)\n",
    "#24.5 ms ± 17.8 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
    "#159 ms ± 2.32 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.372111    0.2642311  -0.18252774 -0.7368198  -0.44030386 -0.15214427\n",
      " -0.6713536  -0.59086424  0.73168874  0.56730247]\n"
     ]
    }
   ],
   "source": [
    "def predict(params, input_vec):\n",
    "  assert input_vec.ndim == 1\n",
    "  activations = input_vec\n",
    "  for W, b in params:\n",
    "    outputs = jnp.dot(W, activations) + b  # `activations` on the right-hand side!\n",
    "    activations = jnp.tanh(outputs)\n",
    "  return outputs\n",
    "\n",
    "from functools import partial\n",
    "#predictions = jnp.stack(list(map(partial(predict, params), input_batch)))\n",
    "\n",
    "\n",
    "from jax import vmap\n",
    "#predictions = vmap(partial(predict, params))(input_batch)\n",
    "# or, alternatively\n",
    "#predictions = vmap(predict, in_axes=(None, 0))(params, input_batch)\n",
    "\n",
    "#per_example_gradients = vmap(partial(grad(loss), params))(inputs, targets)\n",
    "\n",
    "\n",
    "#https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)\n",
    "#[-0.372111    0.2642311  -0.18252774 -0.7368198  -0.44030386 -0.15214427\n",
    "# -0.6713536  -0.59086424  0.73168874  0.56730247]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274 ms ± 5.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU\n",
    "#267 ms ± 3.53 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436 ms ± 9.92 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()\n",
    "#299 ms ± 4.18 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310 ms ± 11.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from jax import device_put\n",
    "\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "x = device_put(x)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()\n",
    "#293 ms ± 9.78 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229 ms ± 4.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit np.dot(x, x.T)\n",
    "#224 ms ± 7.34 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.93 ms ± 352 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()\n",
    "#7.77 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3 ms ± 40.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()\n",
    "#1.57 ms ± 26.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661197 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "def sum_logistic(x):\n",
    "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))\n",
    "#[0.25       0.19661197 0.10499357]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_finite_differences(f, x):\n",
    "  eps = 1e-3\n",
    "  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                   for v in jnp.eye(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24998187 0.1964569  0.10502338]\n",
      "-0.035325594\n"
     ]
    }
   ],
   "source": [
    "print(first_finite_differences(sum_logistic, x_small))\n",
    "#[0.24998187 0.1964569  0.10502338]\n",
    "\n",
    "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))\n",
    "#-0.035325594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "4.09 ms ± 296 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "#from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "  return jit(jacfwd(jacrev(fun)))\n",
    "\n",
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "  return jnp.dot(mat, v)\n",
    "\n",
    "def naively_batched_apply_matrix(v_batched):\n",
    "  return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()\n",
    "#Naively batched\n",
    "#5.28 ms ± 1.59 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually batched\n",
      "57.2 µs ± 556 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def batched_apply_matrix(v_batched):\n",
    "  return jnp.dot(v_batched, mat.T)\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()\n",
    "#Manually batched\n",
    "#76.3 µs ± 872 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-vectorized with vmap\n",
      "75.5 µs ± 2.01 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "  return vmap(apply_matrix)(v_batched)\n",
    "\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()\n",
    "#Auto-vectorized with vmap\n",
    "#94.4 µs ± 612 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
